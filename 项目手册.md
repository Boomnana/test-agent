# Test Report Agent 项目手册

## 1. 项目概览

**Test Report Agent** 是一个基于 LLM（大语言模型）的自动化测试报告分析系统：

- 输入：一份或多份测试结果 Excel（包含用例、步骤、预期、实际、结果等）
- 处理：通过一条异步流水线完成数据清洗、模块打标、结果审计、缺陷分析、缺陷聚类和统计
- 输出：
  - HTML 测试报告（交互式，带图表和附录）——存放于 `reports/`
  - 或本地 JSON 结果文件（CLI 模式）

核心价值：

- 自动代替人工编写测试总结与缺陷分析
- 检测“假成功”（False Positive）用例，提升质量审计能力
- 通过 LLM 自动适配各种 Excel 格式与结果字段

---

## 2. 运行方式总览

### 2.1 一键启动前后端（推荐开发/体验）

脚本位置：[start_all.bat](file:///d:/project/test-agent/start_all.bat)

```bat
@echo off
setlocal
set "ROOT=%~dp0"
start "Backend" cmd /c "cd /d %ROOT%backend && uvicorn app.main:app --reload"
start "Frontend" cmd /c "cd /d %ROOT%frontend && npm install && npm run dev"
echo Backend: http://127.0.0.1:8000
echo Frontend: http://localhost:5173
start "" "http://127.0.0.1:8000/api/v1/docs"
start "" "http://localhost:5173/"
exit /b 0
```

- 启动后端 API（FastAPI，热重载）
- 启动前端 SPA（Vite 开发服务器）
- 自动打开：
  - 接口文档：http://127.0.0.1:8000/api/v1/docs
  - 前端控制台：http://localhost:5173/

### 2.2 后端 API 独立运行

见 [README.md](file:///d:/project/test-agent/README.md#L20-L27)：

```bash
cd backend
uvicorn app.main:app --reload
# 或 README 中命令：
# uvicorn app.main:app --app-dir backend --reload
```

- API 根路径：`http://127.0.0.1:8000`
- 业务 API 前缀：`/api/v1`（由配置控制）
- 报告静态访问：`/reports/{report_id}.html`（挂载在同一后端）

### 2.3 前端 SPA 独立运行

目录：[frontend](file:///d:/project/test-agent/frontend)

```bash
cd frontend
npm install
npm run dev
# 默认 http://localhost:5173
```

- Vite 代理 `/api` 到 `http://127.0.0.1:8000`
- 通过前端页面完成登录、上传 Excel、轮询任务状态等

### 2.4 CLI 模式：本地 Excel 一键分析

脚本位置：[analyze_local.py](file:///d:/project/test-agent/analyze_local.py)

```bash
pip install -r requirements.txt

python analyze_local.py "测试用例汇总.xlsx"
# 或指定路径：
# python analyze_local.py "C:\path\to\your\test_data.xlsx"
```

执行结果：

- 控制台输出流水线每一步进度
- 在 `reports/` 目录生成 `result_JOB-xxxx.json`
- 同时统计并输出本次 LLM 总 Token 消耗

---

## 3. 整体架构

### 3.1 逻辑流水线（Pipeline）

核心流水线与 [SYSTEM_DOCUMENTATION.md](file:///d:/project/test-agent/SYSTEM_DOCUMENTATION.md#L23-L34) 一致：

1. **Excel 输入**（上传或 CLI 指定）
2. **数据接入与清洗 (Ingest)**
3. **智能模块打标 (Module Tagging)**
4. **结果审计 (Result Audit)** —— 检查“假成功”
5. **统计分析 (Stats)**
6. **缺陷提取 (Defect Extraction)**
7. **缺陷聚类 (Clustering)**
8. **报告生成 (Reporting)** —— HTML 或 JSON

### 3.2 技术栈

- 语言：Python 3.10+
- 后端：FastAPI
- 任务队列：Celery + Redis（目前主要 pipeline 已有本地 async 版本）
- LLM：智谱 GLM-4-Air（通过 `ZhipuAI` SDK）
- ORM：SQLAlchemy（Async）
- 模板引擎：Jinja2（生成 HTML 报告）
- 前端：React + TypeScript + Vite + Tailwind CSS
- 可视化：Plotly.js（在报告模板中使用）

### 3.3 代码结构

参考 [SYSTEM_DOCUMENTATION.md](file:///d:/project/test-agent/SYSTEM_DOCUMENTATION.md#L115-L135) 和实际目录：

```text
test-agent/
├── analyze_local.py                 # 本地 CLI 分析入口
├── .env                             # LLM Key、模型等配置
├── requirements.txt                 # Python 依赖
├── backend/
│   └── app/
│       ├── main.py                  # FastAPI 入口，挂载路由 & 静态报告
│       ├── core/                    # 配置 & 日志 & 安全
│       ├── db/                      # DB 会话 & Base
│       ├── models/                  # TestCase/Defect/Job 等模型
│       ├── api/                     # API Router + endpoints
│       │   └── endpoints/
│       │       ├── upload.py        # 当前主入口：上传 & 本地流水线
│       │       └── auth.py          # 登录（目前在 router 中被注释）
│       ├── services/                # 核心业务逻辑
│       │   ├── ingest/              # 数据接入（含 LLM 列映射 & 结果归一化）
│       │   ├── audit/               # 结果审计（假成功检测）
│       │   ├── defects/             # 缺陷分析 & 聚类（LLM）
│       │   ├── analytics/           # 统计计算
│       │   ├── report_gen/          # 报告渲染 & LLM 总结文本
│       │   └── llm/                 # LLM Client 封装
│       └── workers/                 # Celery 配置 & 任务
├── frontend/
│   ├── index.html                   # 前端入口 HTML
│   └── src/
│       ├── main.tsx                 # React 挂载入口
│       └── App.tsx                  # 前端控制台页面
└── reports/                         # 自动生成的 HTML / JSON 报告
```

---

## 4. 后端执行流程

### 4.1 FastAPI 应用初始化

文件：[backend/app/main.py](file:///d:/project/test-agent/backend/app/main.py)

- 创建应用：

  ```python
  app = FastAPI(
      title=settings.PROJECT_NAME,
      openapi_url=f"{settings.API_V1_STR}/openapi.json",
      docs_url=f"{settings.API_V1_STR}/docs",
      redoc_url=f"{settings.API_V1_STR}/redoc",
  )
  ```

- CORS：
  - 使用 `settings.BACKEND_CORS_ORIGINS` 配置允许的前端域名
- 路由挂载：
  - `app.include_router(api_router, prefix=settings.API_V1_STR)`
- 报告静态目录：
  - 将项目根目录下的 `reports/` 挂载为 `/reports` 静态路径
- 根接口 `/`：
  - 返回简单 JSON：`{"message": "Welcome to Test Report Agent API"}`
- 全局异常处理：
  - 任意未捕获异常将返回统一格式：

    ```json
    { "error": { "code": 500, "message": "<异常信息>" } }
    ```

### 4.2 API 路由结构

文件：[backend/app/api/api.py](file:///d:/project/test-agent/backend/app/api/api.py)

- 当前启用的路由：
  - `/api/v1/jobs/...` 来自 `upload.router`
- 登录路由：
  - `auth` 模块存在，但在 `api_router` 中被注释掉（与前端代码稍有不一致，见后文）

### 4.3 上传与流水线入口

文件：[backend/app/api/endpoints/upload.py](file:///d:/project/test-agent/backend/app/api/endpoints/upload.py)

#### 4.3.1 上传接口：`POST /api/v1/jobs/upload`

- 接收参数：
  - `file`: `UploadFile`（Excel）
- 处理流程：

  1. 生成 `job_id = uuid4()`
  2. 将文件保存到项目根目录 `uploads/{job_id}_{原文件名}`
  3. 在内存字典中初始化：
     - `job_logs[job_id]`：流水线步骤日志
     - `job_meta[job_id]`：状态 + 报告 URL + 错误信息
  4. 记录“文件已上传”的日志
  5. **使用 `asyncio.create_task` 启动本地异步流水线**：
     - `run_local_pipeline(job_id, file_path)`
  6. 立即返回：

     ```json
     {
       "job_id": "<uuid>",
       "message": "本地流水线已启动。"
     }
     ```

#### 4.3.2 本地流水线：`run_local_pipeline`

函数位置同文件中 [run_local_pipeline](file:///d:/project/test-agent/backend/app/api/endpoints/upload.py#L51-L99)。

完整步骤（与 CLI 模式类似）：

1. **解析 Excel 数据**
   - 调用 `ingest_service.parse_excel(file_path, job_id)`
   - 逐 Sheet 解析，调用 LLM 做列映射 & 结果归一化
   - 组装为 `TestCase` 列表
2. **模块打标（并发 LLM）**
   - `module_tagger.tag_cases_concurrently(cases)`
   - 批量发送 case 给 LLM，识别业务模块
3. **结果审计（并发 LLM）**
   - 创建 `ResultAuditor()`
   - `audit_cases_concurrently` 对所有 `normalized_result == "Pass"` 用例进行审计
   - 标记可能的“假成功”（`audit_status == "Flagged"`）
4. **统计分析**
   - `stats = stats_service.compute_stats(cases)`
   - 统计总用例数、各结果分布、通过率、模块分布、失败 Top 模块等
5. **缺陷事实提取（并发 LLM）**
   - `defects = await defect_extractor.extract_defect_facts_concurrently(cases)`
   - 对 Fail/Blocked 用例提取现象、假设原因、复现步骤等
   - 将 `case.defect_analysis` 反向挂到对应用例上
6. **缺陷聚类 & 报告生成**
   - `clusters = await defect_clusterer.cluster_and_summarize_async(linked_defects, job_id)`
   - 在项目根 `reports/` 下生成 `report_{job_id}.html`
   - 使用 `report_generator.render_report` 渲染模板并写入文件
   - 更新 `job_meta[job_id]`：
     - `status = "completed"`
     - `report_url = "/reports/report_{job_id}.html"`

若出现异常：

- `job_meta[job_id]["status"] = "failed"`
- `job_meta[job_id]["error"] = 异常字符串`
- 在 `job_logs` 中追加错误日志

#### 4.3.3 任务状态查询：`GET /api/v1/jobs/status/{job_id}`

- 返回：

  ```json
  {
    "job_id": "...",
    "status": "pending|running|completed|failed|unknown",
    "logs": ["步骤日志..."],
    "report_url": "/reports/....html" 或 null,
    "error": "错误信息或 null"
  }
  ```

#### 4.3.4 结果跳转：`GET /api/v1/jobs/result/{job_id}`

- 若存在 `report_url`，通过 `RedirectResponse` 重定向到对应 HTML 报告
- 不存在则返回 404 风格的 JSON 错误

### 4.4 LLM 客户端封装

文件：[backend/app/services/llm/client.py](file:///d:/project/test-agent/backend/app/services/llm/client.py)

职责：

- 包装 `ZhipuAI` 的 `chat.completions.create`
- 统一模型配置、最大 Token、重试策略（`tenacity`）
- 支持同步和异步调用：
  - `chat_completion(...)`
  - `achat_completion(...)`（内部用 `asyncio.to_thread` 调用同步版本）
- 处理 “模型把结果包在 ```json``` 代码块里” 等情况，抽取纯 JSON 片段
- 统计本次会话总 Token 消耗（用于 CLI 输出）

---

## 5. 前端执行流程

文件：[frontend/src/App.tsx](file:///d:/project/test-agent/frontend/src/App.tsx)

### 5.1 登录流程（当前与后端状态说明）

- 常量：`const API_BASE = '/api/v1'`
- 登录表单默认值：`admin/admin`
- 点击“登录”时：

  ```ts
  const res = await fetch(`${API_BASE}/auth/login`, { ... })
  ```

  期望从后端拿到 `access_token` 并存入 `token` 状态。

> 注意：当前后端的 `auth` 路由在 `api_router` 中被注释掉，因此登录 API 可能是历史遗留。可以：
> - 要么重新启用后端 `/auth/login` 路由
> - 要么简化前端逻辑，直接调用无需 JWT 的 `/jobs/upload`（当前 upload 接口也没有检查 Authorization）

### 5.2 上传与任务轮询

1. 用户选择 `.xlsx` 文件并点击“上传并启动分析”
2. 前端发送 `POST /api/v1/jobs/upload`，附带 FormData（以及当前代码中附带的 `Authorization`）
3. 接口返回 `job_id` 后，前端开始定时轮询：

   ```ts
   setInterval(async () => {
     const res = await fetch(`${API_BASE}/jobs/status/${id}`)
     const data = await res.json()
     setStatus(data)
   }, 2000)
   ```

4. 界面上显示完整的 `status` JSON，包含日志及 `report_url`

### 5.3 结果查看

- `status.report_url` 指向后端静态 HTML 报告，如：`/reports/report_<job_id>.html`
- 当前 `openResult` 函数以 JSON 方式拉取并展示，这是与后端 HTML 报告略有不一致的一点。
- 实际使用中，更自然的做法是直接 `window.open(status.report_url, '_blank')` 在浏览器中打开报告。

---

## 6. 提示词（Prompt）总览与位置

本项目的智能行为几乎都由 LLM Prompt 决定，主要分布在以下模块中。

### 6.1 数据接入：列名对齐 Prompt

位置：[ingest/service.py · _align_columns_with_llm](file:///d:/project/test-agent/backend/app/services/ingest/service.py#L44-L120)

**用途：**

- 将任意 Excel 表头映射到系统标准字段（如 `case_name`, `steps`, `expected`, `actual`, `test_result` 等）

**核心指令（节选）：**

```text
你是一个数据解析引擎。你的任务是将输入的 Excel 列名映射到标准字段。

【重要指令】
1. 仅输出纯 JSON 字符串。
2. 严禁输出任何 Python 代码、Markdown 标记（如 ```json）、解释或思考过程。
3. 如果输出包含非 JSON 内容，任务将失败。

标准字段说明：
- case_name: 用例名称/标题 (必选)
- precondition: 前置条件
- steps: 测试步骤
- expected: 预期结果
- actual: 实际结果
- test_result: 测试结果/状态 (必选)
- priority: 优先级
- executor: 执行人
- remark: 备注
...
请返回 JSON 对象，键为表格中的原始列名，值为对应的标准字段名。
```

**调整建议：**

- 想增强对你公司内部字段命名的识别，可在“标准字段说明”下新增一些示例映射规则。
- 想更严格限制输出类型，可进一步强调“不得添加未在标准字段中的值”。

---

### 6.2 数据接入：结果归一化 Prompt

位置：[ingest/service.py · _normalize_results_with_llm](file:///d:/project/test-agent/backend/app/services/ingest/service.py#L121-L168)

**用途：**

- 将各种结果描述归一化为标准状态：`Pass / Fail / Blocked / Skipped`

**核心指令（节选）：**

```text
请将以下测试结果值映射到标准状态：Pass, Fail, Blocked, Skipped。

【重要指令】
1. 仅输出纯 JSON 字符串。
2. 严禁输出 Python 代码或 Markdown。

输入值列表：[...不同的结果字符串...]

规则：
- 成功/通过/OK/Success -> Pass
- 失败/错误/Fail/Error/Bug -> Fail
- 阻塞/Block/Blocked -> Blocked
- 跳过/不适用/Skip/NA -> Skipped

示例输出：
{
  "通过": "Pass",
  "bug": "Fail"
}
```

**调整建议：**

- 可以加入你们内部常见的结果字符串，例如 “已验证通过”、“未执行”等，补充到规则说明中。

---

### 6.3 结果审计 Prompt（假成功检测）

位置：[audit/auditor.py · _build_audit_prompt](file:///d:/project/test-agent/backend/app/services/audit/auditor.py#L63-L104)

**用途：**

- 针对所有认为 `Pass` 的用例做二次审查，找出“假成功”。

**核心指令（节选）：**

```text
你是一名严格的测试质量审计员（QA Auditor）。你的任务是审查以下被标记为“成功（Pass）”的测试用例，判断其是否为“假成功（False Positive）”。

请仔细对比【预期结果】、【实际结果】和【备注】，如果发现以下情况，请将其标记为“Flagged”（存疑）：
1. 实际结果明确描述了失败、错误、未找到、不匹配、异常等情况，但状态却为 Pass。
2. 实际结果与预期结果明显矛盾（例如：预期显示A，实际显示B）。
3. 实际结果为空（None/Null）或仅为占位符，无法证明测试通过。
4. 备注中包含“失败”、“Bug”、“缺陷”等关键词。

如果用例确实通过，请标记为“Pass”。

输入用例列表 (JSON): [...若干用例...]

请返回一个 JSON 对象，格式如下：
{
  "results": [
    {
      "id": "用例ID",
      "status": "Pass" 或 "Flagged",
      "reason": "如果是Flagged，请简要说明理由（中文）；如果是Pass，留空。"
    }
  ]
}

注意：
- 严禁返回任何 Python 代码块或 Markdown 格式。
- 仅返回纯 JSON 字符串。
```

**调整建议：**

- 如果你希望更加“保守”或“激进”，可以在前面明确要求“宁可多报/少报 Flagged 的比例”。
- 可以增加对特定关键字的敏感度说明，例如“如果备注中包含 XXX，一定标记为 Flagged”。

---

### 6.4 缺陷事实抽取 Prompt

位置：[defects/extractor.py · _extract_single_defect_async](file:///d:/project/test-agent/backend/app/services/defects/extractor.py#L26-L75)

**用途：**

- 从失败用例中抽取结构化的缺陷信息：现象、事实、假设原因、证据、复现步骤、严重性。

**核心指令（节选）：**

```text
分析此失败用例并提取缺陷事实。

【重要指令】
1. 仅输出纯 JSON 字符串。
2. 严禁输出 Python 代码或 Markdown。
3. 使用中文。
4. 注意：如果在 JSON 值中引用包含双引号的内容，请务必进行转义，或者将其替换为单引号，确保 JSON 格式合法。

用例: {case.case_name}
步骤: {case.steps}
预期结果: {case.expected}
实际结果: {case.actual}
备注: {case.remark}

JSON 结构:
{
  "phenomenon": "简要描述（中文）",
  "observed_fact": "客观事实（中文）",
  "hypothesis": "推测原因（中文）",
  "evidence": ["证据文本"],
  "repro_steps": "复现步骤（中文）",
  "severity_guess": "Critical/Major/Minor"
}
```

**调整建议：**

- 可以要求 “phenomenon” 更面向业务方易读，“hypothesis” 偏技术。
- 可增加对“severity_guess”的定义（例如什么样的场景属于 Critical）。

---

### 6.5 缺陷聚类 Prompt

位置：[defects/clustering.py · cluster_and_summarize_async](file:///d:/project/test-agent/backend/app/services/defects/clustering.py#L25-L48)

**用途：**

- 按语义相似度对缺陷进行聚类，并为每个聚类生成名称、总结和风险评估。

**核心指令（节选）：**

```text
作为测试专家，请分析以下测试缺陷列表，并将它们根据语义相似性归类到不同的聚类中。

【缺陷列表】
ID: 0 | 现象: ...
ID: 1 | 现象: ...
...

【要求】
1. 识别具有共同特征或根因的缺陷，将其归为一类。
2. 每个缺陷必须且只能属于一个聚类。
3. 如果某个缺陷无法归类，可以单独成一类。
4. 请使用中文回答。

【输出格式】
请仅输出合法的 JSON 字符串，格式如下：
{
  "clusters": [
    {
      "cluster_name": "聚类名称 (简短)",
      "summary": "聚类总结 (描述该类缺陷的共同特征)",
      "risk_assessment": "风险评估 (该类缺陷对系统的潜在影响)",
      "defect_ids": ["ID1", "ID2"]
    }
  ]
}
```

**鲁棒性代码逻辑：**

- 若 LLM 遗漏部分缺陷未归类 → 自动创建“未分类缺陷”聚类
- 若调用彻底失败 → 所有缺陷归入“全部缺陷 (自动聚类失败)”聚类

---

### 6.6 报告执行总结 Prompt

位置：[report_gen/renderer.py · generate_summary](file:///d:/project/test-agent/backend/app/services/report_gen/renderer.py#L12-L46)

**用途：**

- 基于统计数据和缺陷聚类，生成一段 HTML 格式的测试报告总结段落。

**核心指令（节选）：**

```text
基于以下测试数据撰写一份测试报告执行总结：

统计数据: {stats}
缺陷聚类: [cluster_name 列表]
{如果有假成功: "注意：在结果审计中发现了 X 个疑似'假成功'..."}

请重点关注：
1. 整体质量评估。
2. 关键风险领域。
3. 改进建议。
4. (如果有) 数据可信度风险。

【重要指令】
- 直接输出 HTML 段落格式（例如 <p>...</p>）。
- 必须使用中文。
- 严禁输出 Python 代码或 Markdown。
- 不要包含任何其他解释性文字，只输出 HTML 内容。
```

**调整建议：**

- 根据你公司的模板，可以要求增加某些固定小节，比如“版本信息”、“测试范围”等。
- 若想更简洁，可明确“请控制在 3～5 段之内”。

---

## 7. 数据与报告结构概览（高层）

虽然具体模型定义在 [backend/app/models](file:///d:/project/test-agent/backend/app/models)，但从服务调用可以总结核心结构：

- **TestCase**
  - `case_name`, `steps`, `expected`, `actual`, `test_result`, `normalized_result`
  - `module`（通过打标获得）
  - `audit_status`, `audit_reason`
  - `defect_analysis`（指向 DefectAnalysis）
- **DefectAnalysis**
  - `phenomenon`, `observed_fact`, `hypothesis`, `evidence`, `repro_steps`, `severity_guess`
  - `testcase`（反向引用）
  - `cluster`（所属 DefectCluster）
- **DefectCluster**
  - `cluster_name`, `summary`, `risk_assessment`
  - `defects`（该类下的缺陷列表）
- **Stats（dict）**
  - `total_cases`: 用例总数
  - `results`: 各结果数量统计
  - `pass_rate`: 通过率
  - `modules`: 各模块用例数
  - `top_failed_modules`: 前若干失败最集中的模块

HTML 报告模板位于：[backend/app/services/report_gen/templates/report.html](file:///d:/project/test-agent/backend/app/services/report_gen/templates/report.html)

---

## 8. 典型端到端场景

### 8.1 通过前端+后端完成一次分析

1. 运行 `start_all.bat`
2. 浏览器打开：http://localhost:5173/
3.（可选）根据实际后端情况调整前端登录逻辑或先忽略登录
4. 选择一份测试结果 Excel 文件，点击“上传并启动分析”
5. 观察 UI 中的 “任务状态”：
   - 状态从 `pending` → `running` → `completed` 或 `failed`
   - `logs` 中可以看到每一步流水线的中文描述
6. 当状态为 `completed` 且有 `report_url` 时：
   - 再浏览器地址栏直接访问 `http://127.0.0.1:8000` + `report_url`
   - 即可查看生成的 HTML 全量报告

### 8.2 通过 CLI 快速分析本地文件（不依赖前端）

1. 准备 `.env`，填入正确的 `GLM_API_KEY` 和 `LLM_MODEL`
2. 安装依赖：

   ```bash
   pip install -r requirements.txt
   ```

3. 执行：

   ```bash
   python analyze_local.py "测试用例汇总.xlsx"
   ```

4. 等待管道执行完成，控制台将输出：
   - 每一步的中文说明
   - 报告 JSON 路径，如：`reports/result_JOB-xxxx.json`
   - 本次分析总 Token 消耗

---

## 9. 如何快速上手修改

- **修改 Prompt 行为：**
  - 找到上面列出的对应文件（ingest/audit/defects/clustering/report_gen），在函数内编辑 Prompt 文本即可。
- **增加新字段：**
  - 修改 `TestCase` / `DefectAnalysis` 等模型
  - 在 Ingest 的 `_row_to_case_dict` 中补充字段
  - 对应模板 `report.html` 中增加展示
- **切换 LLM 模型或 Key：**
  - 修改 `.env` 中的 `LLM_API_KEY` 和 `LLM_MODEL`
  - 确认 `ZhipuAI` SDK 支持该模型
- **切换运行模式（本地 async vs Celery）：**
  - 当前上传接口直接使用本地 async 流水线
  - 若希望用 Celery，可参考 [workers/tasks.py](file:///d:/project/test-agent/backend/app/workers/tasks.py) 中的 `process_job_pipeline`，再在上传接口中调用 Celery 任务而不是 `asyncio.create_task`.

---

这份手册覆盖了：

- 项目启动和执行的完整流程（前端、后端、CLI）
- LLM 提示词的核心内容与位置
- 整体架构与目录结构

如果你希望，我也可以在这份基础上帮你再拆一份“给非技术同事看的简版说明”。